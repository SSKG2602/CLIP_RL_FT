{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_clip_rl.ipynb\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# Load the pre-trained model and processor\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_model.load_state_dict(torch.load(\"clip_finetuned.pt\")['model_state_dict'])\n",
    "clip_model.to(device)\n",
    "clip_model.train()\n",
    "\n",
    "# Reduce dataset size for RL fine-tuning\n",
    "rl_train_size = int(0.4 * len(train_dataset))  # Adjust train dataset size\n",
    "rl_train_dataset, _ = random_split(train_dataset, [rl_train_size, len(train_dataset) - rl_train_size])\n",
    "rl_train_dataloader = DataLoader(rl_train_dataset, batch_size=6, shuffle=True, collate_fn=preprocess_data)\n",
    "\n",
    "# Improved reward function for RL\n",
    "def reward_function(predictions, ground_truth):\n",
    "    correct = (predictions == ground_truth).float()\n",
    "    confidence = torch.softmax(logits, dim=1).max(dim=1).values\n",
    "    return torch.mean(correct * confidence)\n",
    "\n",
    "# RL fine-tuning loop\n",
    "optimizer = torch.optim.Adam(clip_model.parameters(), lr=1e-5)\n",
    "num_rl_epochs = 3\n",
    "\n",
    "for epoch in range(num_rl_epochs):\n",
    "    total_reward = 0\n",
    "    print(f\"Starting RL epoch {epoch + 1}\")\n",
    "    with tqdm(total=len(rl_train_dataloader), desc=f\"RL Epoch {epoch + 1}\", unit=\"batch\") as pbar:\n",
    "        for step, batch in enumerate(rl_train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = clip_model(**batch)\n",
    "            logits = outputs.logits_per_image\n",
    "            labels = torch.arange(len(logits)).to(device)\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            rewards = reward_function(predictions, labels)\n",
    "            (-rewards).backward()  # Minimize negative rewards\n",
    "            optimizer.step()\n",
    "            total_reward += rewards.item()\n",
    "\n",
    "            pbar.set_postfix(reward=f\"{rewards.item():.4f}\")\n",
    "            pbar.update(1)\n",
    "\n",
    "    print(f\"RL Epoch {epoch + 1}: Average Reward = {total_reward / len(rl_train_dataloader):.4f}\")\n",
    "\n",
    "# Save RL fine-tuned model\n",
    "clip_model.save_pretrained(\"clip_rl_finetuned\")\n",
    "clip_processor.save_pretrained(\"clip_rl_finetuned_processor\")\n",
    "\n",
    "print(\"RL Fine-tuning completed and model saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
